<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PyTorch深度学习</title>
      <link href="/2022/08/22/shen-du-xue-xi-1.0/"/>
      <url>/2022/08/22/shen-du-xue-xi-1.0/</url>
      
        <content type="html"><![CDATA[<h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><h4 id="随机梯度下降法和梯度下降法的主要区别在于："><a href="#随机梯度下降法和梯度下降法的主要区别在于：" class="headerlink" title="随机梯度下降法和梯度下降法的主要区别在于："></a>随机梯度下降法和梯度下降法的主要区别在于：</h4><ul><li>损失函数由cost()更改为loss()。cost是计算所有训练数据的损失，loss是计算一个训练函数的损失。对应于源代码则是少了两个for循环。</li><li>梯度函数gradient()由计算所有训练数据的梯度更改为计算一个训练数据的梯度</li><li>随机梯度主要是指，每次拿一个训练数据来训练，然后更新梯度参数。</li></ul><h2 id="用PyTorch实现线性回归"><a href="#用PyTorch实现线性回归" class="headerlink" title="用PyTorch实现线性回归"></a>用PyTorch实现线性回归</h2><h4 id="PyTorch-Fashion-风格-："><a href="#PyTorch-Fashion-风格-：" class="headerlink" title="PyTorch Fashion(风格)："></a>PyTorch Fashion(风格)：</h4><ul><li>prepare dataset  (准备数据集)</li><li>design model using Class （目的是为了前向传播forward，即计算y hat(预测值)）</li><li>Construct loss and optimizer (using PyTorch API) 其中，计算loss是为了进行反向传播，optimizer是为了更新梯度。</li><li>Training cycle (forward,backward,update)</li></ul><pre class="line-numbers language-python" data-language="python"><code class="language-python">import  torchx_data = torch.Tensor([[1.0],[2.0],[3.0]])y_data = torch.Tensor([[2.0],[4.0],[6.0]])class LinearModel(torch.nn.Module):    def __init__(self):#构造函数        super(LinearModel,self).__init__()        self.linear = torch.nn.Linear(1,1)#构造对象，并说明输入输出的维数，第三个参数默认为true，表示用到b    def forward(self, x):        y_pred = self.linear(x)#可调用对象，计算y=wx+b        return  y_predmodel = LinearModel()#实例化模型criterion = torch.nn.MSELoss(size_average=False)#model.parameters()会扫描module中的所有成员，如果成员中有相应权重，那么都会将结果加到要训练的参数集合上optimizer = torch.optim.SGD(model.parameters(),lr=0.01)#lr为学习率for epoch in range(1000):    y_pred = model(x_data)    loss = criterion(y_pred,y_data)    print(epoch,loss.item())    optimizer.zero_grad()    loss.backward()    optimizer.step()print('w=',model.linear.weight.item())print('b=',model.linear.bias.item())x_test = torch.Tensor([[4.0]])y_test = model(x_test)print('y_pred = ', y_test.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h5 id="代码说明："><a href="#代码说明：" class="headerlink" title="代码说明："></a>代码说明：</h5><p>1、Module实现了魔法函数__call__()，call()里面有一条语句是要调用forward()。因此新写的类中需要重写forward()覆盖掉父类中的forward()</p><p>2、call函数的另一个作用是可以直接在对象后面加()，例如实例化的model对象，和实例化的linear对象</p><p>3、本算法的forward体现是通过以下语句实现的：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_pred = model(x_data)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>由于魔法函数call的实现,model(x_data)将会调用model.forward(x_data)函数，model.forward(x_data)函数中的</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">y_pred = self.linear(x)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>self.linear(x)也由于魔法函数call的实现将会调用torch.nn.Linear类中的forward，至此完成封装，也就是说forward最终是在torch.nn.Linear类中实现的，具体怎么实现，可以不用关心，大概就是y= wx + b。</p><p>4、本算法的反向传播，计算梯度是通过以下语句实现的：</p><p>loss.backward() # 反向传播，计算梯度</p><p>5、本算法的参数(w,b)更新，是通过以下语句实现的：</p><p>optimizer.step() # update 参数，即更新w和b的值</p><p>6、 每一次epoch的训练过程，总结就是</p><p>①前向传播，求y hat （输入的预测值）</p><p>②根据y_hat和y_label(y_data)计算loss</p><p>③反向传播 backward (计算梯度)</p><p>④根据梯度，更新参数</p><p>7、本实例是批量数据处理，小伙伴们不要被optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)误导了，以为见了SGD就是随机梯度下降。要看传进来的数据是单个的还是批量的。这里的x_data是3个数据，是一个batch，调用的PyTorch API是 torch.optim.SGD，但这里的SGD不是随机梯度下降，而是批量梯度下降。也就是说，梯度下降算法使用的是随机梯度下降，还是批量梯度下降，还是mini-batch梯度下降，用的API都是 torch.optim.SGD。</p><p>8、torch.nn.MSELoss也跟torch.nn.Module有关，参与计算图的构建，torch.optim.SGD与torch.nn.Module无关，不参与构建计算图。</p><h2 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h2><ul><li>逻辑斯蒂回归和线性模型的明显区别是在线性模型的后面，添加了<a href="https://so.csdn.net/so/search?q=%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0&amp;spm=1001.2101.3001.7020">激活函数</a>(非线性变换) </li><li> 分布的差异：KL散度，cross-entropy交叉熵</li></ul><p><img src="https://img-blog.csdnimg.cn/2020111320173937.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torch# import torch.nn.functional as F # prepare datasetx_data = torch.Tensor([[1.0], [2.0], [3.0]])y_data = torch.Tensor([[0], [0], [1]]) #design model using classclass LogisticRegressionModel(torch.nn.Module):    def __init__(self):        super(LogisticRegressionModel, self).__init__()        self.linear = torch.nn.Linear(1,1)     def forward(self, x):        # y_pred = F.sigmoid(self.linear(x))        y_pred = torch.sigmoid(self.linear(x))        return y_predmodel = LogisticRegressionModel() # construct loss and optimizer# 默认情况下，loss会基于element平均，如果size_average=False的话，loss会被累加。criterion = torch.nn.BCELoss(size_average = False) optimizer = torch.optim.SGD(model.parameters(), lr = 0.01) # training cycle forward, backward, updatefor epoch in range(1000):    y_pred = model(x_data)    loss = criterion(y_pred, y_data)    print(epoch, loss.item())     optimizer.zero_grad()    loss.backward()    optimizer.step() print('w = ', model.linear.weight.item())print('b = ', model.linear.bias.item()) x_test = torch.Tensor([[4.0]])y_test = model(x_test)print('y_pred = ', y_test.data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="处理多维特征输入"><a href="#处理多维特征输入" class="headerlink" title="处理多维特征输入"></a>处理多维特征输入</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">import numpy as npimport torchimport matplotlib.pyplot as plt # prepare datasetxy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)x_data = torch.from_numpy(xy[:, :-1]) # 第一个‘：’是指读取所有行，第二个‘：’是指从第一列开始，最后一列不要y_data = torch.from_numpy(xy[:, [-1]]) # [-1] 最后得到的是个矩阵 # design model using class  class Model(torch.nn.Module):    def __init__(self):        super(Model, self).__init__()        self.linear1 = torch.nn.Linear(8, 6) # 输入数据x的特征是8维，x有8个特征        self.linear2 = torch.nn.Linear(6, 4)        self.linear3 = torch.nn.Linear(4, 1)        self.sigmoid = torch.nn.Sigmoid() # 将其看作是网络的一层，而不是简单的函数使用     def forward(self, x):        x = self.sigmoid(self.linear1(x))        x = self.sigmoid(self.linear2(x))        x = self.sigmoid(self.linear3(x)) # y hat        return x  model = Model() # construct loss and optimizer# criterion = torch.nn.BCELoss(size_average = True)criterion = torch.nn.BCELoss(reduction='mean')  optimizer = torch.optim.SGD(model.parameters(), lr=0.1) epoch_list = []loss_list = []# training cycle forward, backward, updatefor epoch in range(100):    y_pred = model(x_data)    loss = criterion(y_pred, y_data)    print(epoch, loss.item())    epoch_list.append(epoch)    loss_list.append(loss.item())     optimizer.zero_grad()    loss.backward()     optimizer.step()  plt.plot(epoch_list, loss_list)plt.ylabel('loss')plt.xlabel('epoch')plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/20201113211329882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><h4 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h4><p>1、乘的权重(w)都一样，加的偏置(b)也一样。b变成矩阵时使用广播机制。神经网络的参数w和b是网络需要学习的，其他是已知的。</p><p>2、学习能力越强，有可能会把输入样本中噪声的规律也学到。我们要学习数据本身真实数据的规律，学习能力要有泛化能力。</p><p> 3、该神经网络共3层；第一层是8维到6维的非线性空间变换，第二层是6维到4维的非线性空间变换，第三层是4维到1维的非线性空间变换。</p><p> 4、本算法中torch.nn.Sigmoid() # 将其看作是网络的一层，而不是简单的函数使用 </p><h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><h4 id="说明：-1"><a href="#说明：-1" class="headerlink" title="说明："></a>说明：</h4><ul><li>DataSet 是抽象类，不能实例化对象，主要是用于构造我们的数据集</li><li>DataLoader 需要获取DataSet提供的索引[i]和len;用来帮助我们加载数据，比如说做shuffle(提高数据集的随机性)，batch_size,能拿出Mini-Batch进行训练。它帮我们自动完成这些工作。DataLoader可实例化对象。DataLoader is a class to help us loading data in Pytorch.</li><li>__getitem__目的是为支持下标(索引)操作</li></ul><p><img src="https://img-blog.csdnimg.cn/20201114092441482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport numpy as npfrom torch.utils.data import Datasetfrom torch.utils.data import DataLoader # prepare dataset  class DiabetesDataset(Dataset):    def __init__(self, filepath):        xy = np.loadtxt(filepath, delimiter=',', dtype=np.float32)        self.len = xy.shape[0] # shape(多少行，多少列)        self.x_data = torch.from_numpy(xy[:, :-1])        self.y_data = torch.from_numpy(xy[:, [-1]])     def __getitem__(self, index):        return self.x_data[index], self.y_data[index]     def __len__(self):        return self.len  dataset = DiabetesDataset('diabetes.csv')train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=2) #num_workers 多线程  # design model using class  class Model(torch.nn.Module):    def __init__(self):        super(Model, self).__init__()        self.linear1 = torch.nn.Linear(8, 6)        self.linear2 = torch.nn.Linear(6, 4)        self.linear3 = torch.nn.Linear(4, 1)        self.sigmoid = torch.nn.Sigmoid()     def forward(self, x):        x = self.sigmoid(self.linear1(x))        x = self.sigmoid(self.linear2(x))        x = self.sigmoid(self.linear3(x))        return x  model = Model() # construct loss and optimizercriterion = torch.nn.BCELoss(reduction='mean')optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # training cycle forward, backward, updateif __name__ == '__main__':    for epoch in range(100):        for i, data in enumerate(train_loader, 0): # train_loader 是先shuffle后mini_batch            inputs, labels = data            y_pred = model(inputs)            loss = criterion(y_pred, labels)            print(epoch, i, loss.item())             optimizer.zero_grad()            loss.backward()             optimizer.step()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="代码说明：-1"><a href="#代码说明：-1" class="headerlink" title="代码说明："></a>代码说明：</h4><p>1、需要mini_batch 就需要import DataSet和DataLoader</p><p>2、继承DataSet的类需要重写init，getitem,len魔法函数。分别是为了加载数据集，获取数据索引，获取数据总量。</p><p>3、DataLoader对数据集先打乱(shuffle)，然后划分成mini_batch。</p><p>4、len函数的返回值 除以 batch_size 的结果就是每一轮epoch中需要迭代的次数。</p><p>5、inputs, labels = data中的inputs的shape是[32,8],labels 的shape是[32,1]。也就是说mini_batch在这个地方体现的</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p><a href=""></a><a href="https://blog.csdn.net/bit452/article/details/109686936">(37条消息) PyTorch 深度学习实践 第9讲_错错莫的博客-CSDN博客</a></p><h2 id="卷积神经网络-基础篇"><a href="#卷积神经网络-基础篇" class="headerlink" title="卷积神经网络(基础篇)"></a>卷积神经网络(基础篇)</h2><p>说明 :</p><ul><li>前一部分叫做Feature Extraction，后一部分叫做classification</li><li>每一个卷积核它的通道数量要求和输入通道是一样的。这种卷积核的总数有多少个和你输出通道的数量是一样的。</li><li>卷积(convolution)后，C(Channels)变，W(width)和H(Height)可变可不变，取决于是否padding。subsampling(或pooling)后，C不变，W和H变。</li><li>卷积层：保留图像的空间信息。</li><li>卷积层要求输入输出是四维张量(B,C,W,H)，全连接层的输入与输出都是二维张量(B,Input_feature)。</li><li>卷积(线性变换)，激活函数(非线性变换)，池化；这个过程若干次后，view打平，进入全连接层~</li></ul><p><img src="https://img-blog.csdnimg.cn/20201114144337983.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20201114144345796.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchfrom torchvision import transformsfrom torchvision import datasetsfrom torch.utils.data import DataLoaderimport torch.nn.functional as Fimport torch.optim as optim # prepare dataset batch_size = 64transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) # design model using class  class Net(torch.nn.Module):    def __init__(self):        super(Net, self).__init__()        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)        self.pooling = torch.nn.MaxPool2d(2)        self.fc = torch.nn.Linear(320, 10)      def forward(self, x):        # flatten data from (n,1,28,28) to (n, 784)        batch_size = x.size(0)        x = F.relu(self.pooling(self.conv1(x)))        x = F.relu(self.pooling(self.conv2(x)))        x = x.view(batch_size, -1) # -1 此处自动算出的是320        x = self.fc(x)         return x  model = Net() # construct loss and optimizercriterion = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) # training cycle forward, backward, update  def train(epoch):    running_loss = 0.0    for batch_idx, data in enumerate(train_loader, 0):        inputs, target = data        optimizer.zero_grad()         outputs = model(inputs)        loss = criterion(outputs, target)        loss.backward()        optimizer.step()         running_loss += loss.item()        if batch_idx % 300 == 299:            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))            running_loss = 0.0  def test():    correct = 0    total = 0    with torch.no_grad():        for data in test_loader:            images, labels = data            outputs = model(images)            _, predicted = torch.max(outputs.data, dim=1)            total += labels.size(0)            correct += (predicted == labels).sum().item()    print('accuracy on test set: %d %% ' % (100*correct/total))  if __name__ == '__main__':    for epoch in range(10):        train(epoch)        test()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="GPU版本"><a href="#GPU版本" class="headerlink" title="GPU版本"></a>GPU版本</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchfrom torchvision import transformsfrom torchvision import datasetsfrom torch.utils.data import DataLoaderimport torch.nn.functional as Fimport torch.optim as optimimport matplotlib.pyplot as plt # prepare dataset batch_size = 64transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) # design model using class  class Net(torch.nn.Module):    def __init__(self):        super(Net, self).__init__()        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)        self.pooling = torch.nn.MaxPool2d(2)        self.fc = torch.nn.Linear(320, 10)      def forward(self, x):        # flatten data from (n,1,28,28) to (n, 784)                batch_size = x.size(0)        x = F.relu(self.pooling(self.conv1(x)))        x = F.relu(self.pooling(self.conv2(x)))        x = x.view(batch_size, -1) # -1 此处自动算出的是320        # print("x.shape",x.shape)        x = self.fc(x)         return x  model = Net()device = torch.device("cuda" if torch.cuda.is_available() else "cpu")model.to(device) # construct loss and optimizercriterion = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) # training cycle forward, backward, update  def train(epoch):    running_loss = 0.0    for batch_idx, data in enumerate(train_loader, 0):        inputs, target = data        inputs, target = inputs.to(device), target.to(device)        optimizer.zero_grad()         outputs = model(inputs)        loss = criterion(outputs, target)        loss.backward()        optimizer.step()         running_loss += loss.item()        if batch_idx % 300 == 299:            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))            running_loss = 0.0  def test():    correct = 0    total = 0    with torch.no_grad():        for data in test_loader:            images, labels = data            images, labels = images.to(device), labels.to(device)            outputs = model(images)            _, predicted = torch.max(outputs.data, dim=1)            total += labels.size(0)            correct += (predicted == labels).sum().item()    print('accuracy on test set: %d %% ' % (100*correct/total))    return correct/total  if __name__ == '__main__':    epoch_list = []    acc_list = []        for epoch in range(10):        train(epoch)        acc = test()        epoch_list.append(epoch)        acc_list.append(acc)        plt.plot(epoch_list,acc_list)    plt.ylabel('accuracy')    plt.xlabel('epoch')    plt.show()     <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="代码说明：-2"><a href="#代码说明：-2" class="headerlink" title="代码说明："></a>代码说明：</h4><p>1、torch.nn.Conv2d(1,10,kernel_size=3,stride=2,bias=False)</p><p>​         ==1是指输入的Channel，灰色图像是1维的；10是指输出的Channel，也可以说第一个卷积层需要10个卷积核；kernel_size=3,卷积核大小是3x3；stride=2进行卷积运算时的步长，默认为1；bias=False卷积运算是否需要偏置bias，默认为False。padding = 0，卷积操作是否补0。==</p><p>2、self.fc = torch.nn.Linear(320, 10)，这个320获取的方式，可以通过x = x.view(batch_size, -1) # print(x.shape)可得到(64,320),64指的是batch，320就是指要进行全连接操作时，输入的特征维度。</p><h2 id="卷积神经网络（高级篇）"><a href="#卷积神经网络（高级篇）" class="headerlink" title="卷积神经网络（高级篇）"></a>卷积神经网络（高级篇）</h2><h4 id="说明：Inception-Moudel"><a href="#说明：Inception-Moudel" class="headerlink" title="说明：Inception Moudel"></a>说明：Inception Moudel</h4><p>1、卷积核超参数选择困难，自动找到卷积的最佳组合。</p><p>2、1x1卷积核，不同通道的信息融合。使用1x1卷积核虽然参数量增加了，但是能够显著的降低计算量(operations)</p><p>3、Inception Moudel由4个分支组成，要分清哪些是在Init里定义，哪些是在forward里调用。4个分支在dim=1(channels)上进行concatenate。24+16+24+24 = 88<br><img src="https://img-blog.csdnimg.cn/20201114173159892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20210109094234494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnfrom torchvision import transformsfrom torchvision import datasetsfrom torch.utils.data import DataLoaderimport torch.nn.functional as Fimport torch.optim as optim # prepare dataset batch_size = 64transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # 归一化,均值和方差 train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) # design model using classclass InceptionA(nn.Module):    def __init__(self, in_channels):        super(InceptionA, self).__init__()        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)         self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)         self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)         self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)     def forward(self, x):        branch1x1 = self.branch1x1(x)         branch5x5 = self.branch5x5_1(x)        branch5x5 = self.branch5x5_2(branch5x5)         branch3x3 = self.branch3x3_1(x)        branch3x3 = self.branch3x3_2(branch3x3)        branch3x3 = self.branch3x3_3(branch3x3)         branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)        branch_pool = self.branch_pool(branch_pool)         outputs = [branch1x1, branch5x5, branch3x3, branch_pool]        return torch.cat(outputs, dim=1) # b,c,w,h  c对应的是dim=1 class Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)        self.conv2 = nn.Conv2d(88, 20, kernel_size=5) # 88 = 24x3 + 16         self.incep1 = InceptionA(in_channels=10) # 与conv1 中的10对应        self.incep2 = InceptionA(in_channels=20) # 与conv2 中的20对应         self.mp = nn.MaxPool2d(2)        self.fc = nn.Linear(1408, 10)       def forward(self, x):        in_size = x.size(0)        x = F.relu(self.mp(self.conv1(x)))        x = self.incep1(x)        x = F.relu(self.mp(self.conv2(x)))        x = self.incep2(x)        x = x.view(in_size, -1)        x = self.fc(x)         return x model = Net() # construct loss and optimizercriterion = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) # training cycle forward, backward, update  def train(epoch):    running_loss = 0.0    for batch_idx, data in enumerate(train_loader, 0):        inputs, target = data        optimizer.zero_grad()         outputs = model(inputs)        loss = criterion(outputs, target)        loss.backward()        optimizer.step()         running_loss += loss.item()        if batch_idx % 300 == 299:            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))            running_loss = 0.0  def test():    correct = 0    total = 0    with torch.no_grad():        for data in test_loader:            images, labels = data            outputs = model(images)            _, predicted = torch.max(outputs.data, dim=1)            total += labels.size(0)            correct += (predicted == labels).sum().item()    print('accuracy on test set: %d %% ' % (100*correct/total))  if __name__ == '__main__':    for epoch in range(10):        train(epoch)        test()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码说明：</p><ul><li><p>先使用类对Inception Moudel进行封装</p></li><li><p>先是1个卷积层(conv,maxpooling,relu)，然后inceptionA模块(输出的channels是24+16+24+24=88)，接下来又是一个卷积层(conv,mp,relu),然后inceptionA模块，最后一个全连接层(fc)。</p></li><li><p>1408这个数据可以通过x = x.view(in_size, -1)后调用x.shape得到。</p><p>​         </p></li></ul><h4 id="要解决的问题：梯度消失"><a href="#要解决的问题：梯度消失" class="headerlink" title="要解决的问题：梯度消失"></a>要解决的问题：梯度消失</h4><p>​    跳连接，H(x) = F(x) + x,张量维度必须一样，加完后再激活。不要做pooling，张量的维度会发生变化。</p><p><img src="https://img-blog.csdnimg.cn/20201114182336225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20210109122742901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JpdDQ1Mg==,size_16,color_FFFFFF,t_70" alt="img"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchimport torch.nn as nnfrom torchvision import transformsfrom torchvision import datasetsfrom torch.utils.data import DataLoaderimport torch.nn.functional as Fimport torch.optim as optim # prepare dataset batch_size = 64transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # 归一化,均值和方差 train_dataset = datasets.MNIST(root='../dataset/mnist/', train=True, download=True, transform=transform)train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)test_dataset = datasets.MNIST(root='../dataset/mnist/', train=False, download=True, transform=transform)test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size) # design model using classclass ResidualBlock(nn.Module):    def __init__(self, channels):        super(ResidualBlock, self).__init__()        self.channels = channels        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)     def forward(self, x):        y = F.relu(self.conv1(x))        y = self.conv2(y)        return F.relu(x + y) class Net(nn.Module):    def __init__(self):        super(Net, self).__init__()        self.conv1 = nn.Conv2d(1, 16, kernel_size=5)        self.conv2 = nn.Conv2d(16, 32, kernel_size=5) # 88 = 24x3 + 16         self.rblock1 = ResidualBlock(16)        self.rblock2 = ResidualBlock(32)         self.mp = nn.MaxPool2d(2)        self.fc = nn.Linear(512, 10) # 暂时不知道1408咋能自动出来的      def forward(self, x):        in_size = x.size(0)         x = self.mp(F.relu(self.conv1(x)))        x = self.rblock1(x)        x = self.mp(F.relu(self.conv2(x)))        x = self.rblock2(x)         x = x.view(in_size, -1)        x = self.fc(x)        return x model = Net() # construct loss and optimizercriterion = torch.nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5) # training cycle forward, backward, update  def train(epoch):    running_loss = 0.0    for batch_idx, data in enumerate(train_loader, 0):        inputs, target = data        optimizer.zero_grad()         outputs = model(inputs)        loss = criterion(outputs, target)        loss.backward()        optimizer.step()         running_loss += loss.item()        if batch_idx % 300 == 299:            print('[%d, %5d] loss: %.3f' % (epoch+1, batch_idx+1, running_loss/300))            running_loss = 0.0  def test():    correct = 0    total = 0    with torch.no_grad():        for data in test_loader:            images, labels = data            outputs = model(images)            _, predicted = torch.max(outputs.data, dim=1)            total += labels.size(0)            correct += (predicted == labels).sum().item()    print('accuracy on test set: %d %% ' % (100*correct/total))  if __name__ == '__main__':    for epoch in range(10):        train(epoch)        test()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码说明：</p><p>1、先是1个卷积层(conv,maxpooling,relu)，然后ResidualBlock模块，接下来又是一个卷积层(conv,mp,relu),然后esidualBlock模块模块，最后一个全连接层(fc)。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/06/05/hello-world/"/>
      <url>/2022/06/05/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new "My New Post"<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
